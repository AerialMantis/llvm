# Matrix Programming Extension for DPC++: SYCL_EXT_ONEAPI_MATRIX
:source-highlighter: coderay
:coderay-linenums-mode: table
:dpcpp: pass:[DPC++]

// This section needs to be after the document title.
:doctype: book
:toc2:
:toc: left
:encoding: utf-8
:lang: en

:blank: pass:[ +]

// Set the default source code type in this document to C++,
// for syntax highlighting purposes.  This is needed because
// docbook uses c++ and html5 uses cpp.
:language: {basebackend@docbook:c++:cpp}


== Notice

Copyright (c) 2021-2021 Intel Corporation.  All rights reserved.

NOTE: Khronos(R) is a registered trademark and SYCL(TM) and SPIR(TM) are
trademarks of The Khronos Group Inc.  OpenCL(TM) is a trademark of Apple Inc.
used by permission by Khronos.

This extension is written against the SYCL 2020 revision 3 specification.  All
references below to the "core SYCL specification" or to section numbers in the
SYCL specification refer to that revision.


**_NOTE:_** _This document describes the current design and API for the matrix
extension to {dpcpp}. This is an initial experimental version to try out functionality
and performance, and **future versions of this API may change in ways that are incompatible with this experimental version**. The current implementation provides support of the matrix interface on Intel(R) Advanced Matrix Extensions (AMX) and DPAS. We are going to work with the community on incrementally improving
the API to bring them closer to standard C++ (aligned with the `std::mdspan` and `std::mdarray` proposals) and SYCL in the next several months._

## Introduction
This document presents an ongoing work towards defining a unified matrix interface. This interface is intended to unify different tensor hardware: AMX in Intel CPU, Habana Gaudi and Goya tensor and gemm cores, Nvidia TPUs, IBM Power MMA. All these hardware provide low-level intrinsics or assembly to access and perform matrix operations. The goal is to provide a unified interface that is portable but also benefit from the maximum performance these different hardware can offer.

## Feature test macro

This extension provides a feature-test macro as described in the core SYCL
specification section 6.3.3 "Feature test macros".  Therefore, an
implementation supporting this extension must predefine the macro
`SYCL_EXT_ONEAPI_MATRIX` to one of the values defined in the table below.
Applications can test for the existence of this macro to determine if the
implementation supports this feature, or applications can test the macro's
value to determine which of the extension's APIs the implementation supports.

[frame="none",options="header"]
|======================
|Value |Description
|1     |Initial extension implementation on AMX.  Base features are supported.
|2     |Initial extension JIT implementation on AMX and DPAS. load, store, mad and the query interface are supported 
|======================

## New `joint_matrix` class
We introduce a new class called `joint_matrix`. The user needs to specify the type of the elements, shape, the memory layout, and the memory scope of the matrix. This results into the following description:

```c++
namespace sycl::ext::oneapi::experimental::matrix {
template <typename T, size_t Rows=sycl::dynamic_extent, size_t Cols=sycl::dynamic_extent, 
          matrix_layout Layout = matrix_layout::row_major, typename Group = sub_group>
struct joint_matrix {
    joint_matrix(Group g) {}
};
}
```


#### Memory Scope
In this experimental API version, we used the terminology of `joint_matrix` instead of plain `matrix` to emphasis that the matrix is shared among a group of work items and is not private to each work item. The memory scope is added as an additional template parameter and is also part of the constructor arguments.

IMPORTANT: In the current implementation, only the subgroup scope is supported

When the group is a `sycl::sub_group`, a matrix is declared as follows:

```c++
joint_matrix<int8_t, tM, tN> tA(sg); 
```

#### Shape
The same class `joint_matrix` should handle both cases where sizes are constant (GPU case) and when sizes are variables (CPU case). Note that a AMX 2d tile register permits sizes up to 1024 (16rowsx64cols) bytes. The ability to define only one interface for both makes it possible to give the user a way to make use of the flexibility introduced by the CPU but at the same time save resources on the GPU. We use `sycl::dynamic_extent`  to differentiate between static and dynamic sizes.

IMPORTANT: In the current implementation, only the static extent is supported


#### Layout
Besides row major and column major layouts, `matrix_layout` is flexible enough to introduce customed layouts such as symmetric or tiled layouts.
	
```c++
namespace sycl::ext::oneapi::experimental::matrix {
enum class matrix_layout {
  row_major,
  col_major,
  packed_a,
  packed_b
};
}
```

AMX and DPAS hardware require B matrix to be in VNNI or 32 bits packed layout. If we multiply matrices A (M, K) and B (K, N) into a matrix C (M, N). The logical sizes are M, K, N. However, the packed shape for B tile uses the VNNI format, which is described below. The user must provide the information of packed_b layout to make the implementation allocate the right shape. The layout information for AMX should be specified in user code as follows: 

```c++
joint_matrix<int8_t, K, N, packed_b> tB(sg);
```   
IMPORTANT: In the current implementation, only `packed_b` layout is necessary to specify on matrix B, the layout on other matrices is ignored.



## Matrix Operations and their Execution Scope
We define three new functions needed to perform the main and common operations on matrices namely, load, store, and the actual multiply and add operation. This set of functions can be easily extended if the tensor hardware implements new features.

The base pointer determines the starting address of the matrix to be loaded/stored. `layout` determines whether the data are being read/written in a row (`row_major`), column major (`column_major`) fashion, or if the data has already been transformed into VNNI format (`packed_a`, `packed_b`). `stride` describes the number of elements between consecutive rows for row major and packed layout,  columns for column major layout. 

Note that for getting maximum performance on AMX and DPAS, prepacking data in the memory is necessary. If users did not specify the packed layouts (`packed_a` in column major case, `packed_b` in row major case), transforms done by the implementation will be slow due to extra scatter/gather operations. Hence, we expose these layouts `packed_a` and `packed_b` to the user to specify that A and/or B have already been VNNIed. The packed or VNNI layout is introduced in `VNNI layout` section below.
	
IMPORTANT: In the current implementation, the layout in the load of matrix B must be `packed_b`.  Therefore, both the template parameter for the declaration of the B matrix and the call to `joint_matrix_load` for the B matrix must specify the `packed_b` layout.  The layout in the load of matrices A and C must be `row_major`, and the layout in the store of matrix C must also be `row_major`.

Since the matrix functions are group operations (as defined in Section 4.17.3 of the SYCL specification), the matrix API has to be accessed by all the work-items in the group in a convergent control flow. The `Group` template argument can be a work-group or a subgroup. These functions will be called once by each work item in the group.

To be aligned with the SYCL 2020 group algorithms, an additional group argument is added to the matrix operations to designate that these functions are collective operations. The {dpcpp} syntax is the following: 

IMPORTANT: In the current implementation, only the subgroup scope is supported.  

#### Load 
```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename T, size_t NumRows, size_t NumCols,
          matrix_layout L,
          access::address_space Space>
  void joint_matrix_load(Group sg, joint_matrix<T, NumRows, NumCols, L, Group> &res,
		    multi_ptr<T, Space> src, size_t stride, matrix_layout memL);
}
```
This function loads data from memory to the 2d tiles/registers of AMX/DPAS.


#### Store 
```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename T, size_t NumRows, size_t NumCols,
          matrix_layout L,
          access::address_space Space>	  
  void joint_matrix_store(Group sg, joint_matrix<T, NumRows, NumCols, L, Group> &res,
		     multi_ptr<T, Space> src, size_t stride, matrix_layout memL);
}
```
This function stores the data from the 2d tiles back to memory.

#### Multiply and Add

```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename Ta, typename Tb, typename Tc,
          std::size_t M, std::size_t K, std::size_t N,
	  matrix_layout La, matrix_layout Lb,
          matrix_layout Lc>
  joint_matrix<Group, Tc, M, N, Lc> joint_matrix_mad(Group sg, joint_matrix<Ta, M, K, La, Group> A,
               joint_matrix<Tb, K, N, Lb, Group> B, joint_matrix<Tc, M, N, Lc, Group> C);
}
```
The matrix multiply and add function performs the multiply operation on the matrices `A` and `B`, accumulate the result with `C` and return the result.


## VNNI/Packed Layout
AMX and DPAS compute assumes register for B tile (src1) to be in VNNI format as they need 32bit of K-data in A and B to be contiguous in memory.
The VNNI blocking factor is 2 in the case of 16-bit types, and it is 4 in the case of 8-bit types. While the current implementation assumes that the matrix has been already packed by the user for performance reasons, the layout information is needed to inform the implementation about this transform.  The following example illustrates how a matrix in `row_major` layout is transformed into the `packed_b` layout for a 16-bit type.

#### Example 1: 16-bit elements
      // Example of a 4 row x 4 column matrix using a 16-bit data element, in row-major layout.
      // Element a1 is contiguous in memory with element b1, etc.
      // ---------------------------------
      // a1, b1, c1, d1
      // a2, b2, c2, d2
      // a3, b3, c3, d3
      // a4, b4, c4, d4
      // ---------------------------------
      // The same matrix reformatted in packed_b layout. 
      // Here, packing of 2 elements is needed to form 32 bits.
      // Element a1 is contiguous in memory with element a2, etc.
      // ---------------------------------
      // a1, a2, b1, b2, c1, c2, d1, d2
      // a3, a4, b3, b4, c3, c4, d3, d4

#### Example 2: 8-bit elements

      // Example of a 4 row x 4 column matrix using a 8-bit data element, in row-major layout.
      // Element a1 is contiguous in memory with element b1, etc.
      // ---------------------------------
      // a1, b1, c1, d1
      // a2, b2, c2, d2
      // a3, b3, c3, d3
      // a4, b4, c4, d4
      // ---------------------------------
      // The same matrix reformatted in packed_b layout.  
      // Here, packing of 4 elements is needed to form 32 bits.
      // Elements a1, a2, a3, a4 are contiguous in memory, etc.
      // ---------------------------------
      // a1, a2, a3, a4, b1, b2, b3, b4, c1, c2, c3, c4, d1, d2, d3, d4


## Example using int8_t type
```c++
using namespace sycl::ext::oneapi::experimental::matrix;

queue q;
range<2> G = {M/tM, N};
range<2> L = {1, SG_SIZE};
int8_t *memA = malloc_shared<int8_t>(M*K, q);
int8_t *memB = malloc_shared<int8_t>(K*N, q);
Int32_t *memC = malloc_shared<int32_t>(M*N, q);
// Assuming memB has already been VNNIed
q.parallel_for(nd_range<2>(G, L), [=](nd_item<2> item)                            
  [[sycl::reqd_sub_group_size(SG_SIZE)]] {
   const auto global_idx = item.get_global_id(0);
   const auto global_idy = item.get_global_id(1);
   const auto sg_startx = global_idx - item.get_local_id(0);
   const auto sg_starty = global_idy - item.get_local_id(1);
   sub_group sg = item.get_sub_group();
   joint_matrix<int8_t, tM, tK> tA(sg);
   // For B, since current implementation does not support non packed layout,
   // users need to specify the packed_b layout
   joint_matrix<int8_t, tK, tN, packed_b> tB(sg);
   joint_matrix<int32_t, tM, tN> tC(sg);
   joint_matrix_load(sg, tC, memC + sg_startx * tM * N + sg_starty/SG_SIZE*tN, N, matrix_layout::row_major);
   for (int k = 0; k < K; k += tk) {
     joint_matrix_load(sg, tA, memA + sg_startx * tM * K + k, K, matrix_layout::row_major);
     joint_matrix_load(sg, tB, memB + k * N + sg_starty/SG_SIZE*tN*4, N*4, matrix_layout::packed_b); // VNNI
     tC = joint_matrix_mad(sg, tA, tB, tC);
   }
   joint_matrix_store(sg, tC, memC + sg_startx * tM * N + sg_starty/SG_SIZE*tN, N, matrix_layout::row_major);
}).wait();
```

== Query Interface
AMX, DPAS and Nvidia TPUs support different sizes and types. 
The query interface is used to validate user code and inform them about supported types, sizes, scope, and layouts by the implementation.
This also offers development and tuning productivity by both scientists and library developers. The query interface we are proposing here is a compile-time query, 
so there will be no runtime errors.   
The query interface proposed here consists of three functionalities:

- At compile time, inform the user whether a specific combination is valid or not.

- Construct the matrices using a default shape if user does not provide a combination 

- General query interface for sizes, types, static/dynamic, scope. This is needed to void padding by the user, for tuning, and efficient code generation if used by a library.

```c++
namespace sycl::ext::oneapi::experimental::matrix {


template<tpu u, typename Ta=void, typename Tb=void, typename Tc=void, int M=0, int N=0, int K=0>
struct tpu_params;

// Valid or not:
// Specialization when both types and sizes are given
template <typename Ta, typename Tb, typename Tc, int M, int N, int K>
struct tpu_params<
    tpu::amx, Ta, Tb, Tc, M, N, K,
    typename std::enable_if<(
        !std::is_same_v<Ta, void> && !std::is_same_v<Tb, void> &&
        !std::is_same_v<Tc, void> && M != 0 && N != 0 && K != 0)>::type> {
  // Validate that parameters are supported
  static_assert(
      (M == 0 && N == 0 && K == 0) ||
          (is_combination_valid_amx<Ta, Tb, Tc>(M, N, K)),
      "Invalid parameters for AMX, query valid types and maximum sizes "
      "using: "
      "tpu_params<tpu::amx> myparams; and then check out myparams.combinations array");


  // if combination is valid, construct the matrices

  static constexpr std::size_t defaultM = (M != 0) ? M : 16;
  static constexpr std::size_t defaultN = (N != 0) ? N : 16;
  static constexpr std::size_t defaultK =
      (K != 0) ? K : ((sizeof(Ta) == 1) ? 64 : 32);

  template <typename Group>
  using joint_matrix_a =
      joint_matrix<Ta, defaultM, defaultK, matrix_layout::row_major, Group>;
  template <typename Group>
  using joint_matrix_b =
      joint_matrix<Tb, defaultK, defaultN, matrix_layout::packed_b, Group>;
  template <typename Group>
  using joint_matrix_c =
      joint_matrix<Tc, defaultM, defaultN, matrix_layout::row_major, Group>;

  bool dynamic_p = false; // should be true in future implementations
                          // because AMX hardware supports dynamic sizes
  uint32_t numtiles = 8;
  scope_t scope = scope_t::sub_group;
};

// Sizes-only query
// Specialization for when only types are given, need to query only sizes
template <typename Ta, typename Tb, typename Tc>
struct tpu_params<tpu::amx, Ta, Tb, Tc, 0, 0, 0,
                  typename std::enable_if<(!std::is_same_v<Ta, void> &&
                                           !std::is_same_v<Tb, void> &&
                                           !std::is_same_v<Tc, void>)>::type> {
  static_assert((are_types_valid_amx<Ta, Tb, Tc>()),
                "Invalid types for AMX, supported types are int8_t, uint8_t, "
                "and bf16 (Note that unsigned short should be used in the"
                "DPC++ code to implement bf16) ");

  // construct the matrices using the default sizes
  static constexpr std::size_t defaultM = 16;
  static constexpr std::size_t defaultN = 16;
  static constexpr std::size_t defaultK = ((sizeof(Ta) == 1) ? 64 : 32);

  template <typename Group>
  using joint_matrix_a =
      joint_matrix<Ta, defaultM, defaultK, matrix_layout::row_major, Group>;
  template <typename Group>
  using joint_matrix_b =
      joint_matrix<Tb, defaultK, defaultN, matrix_layout::packed_b, Group>;
  template <typename Group>
  using joint_matrix_c =
      joint_matrix<Tc, defaultM, defaultN, matrix_layout::row_major, Group>;

  bool dynamic_p = false; // should be true in future implementations because
                          // AMX hardware supports dynamic sizes
  uint32_t numtiles = 8;
  scope_t scope = scope_t::sub_group;
  struct combination {
    uint32_t max_msize;
    uint32_t max_nsize;
    uint32_t max_ksize;
    matrix_type atype;
    matrix_type btype;
    matrix_type ctype;
    uint32_t msize;
    uint32_t nsize;
    uint32_t ksize;
  };
  static constexpr combination combinations[] = {
      {16, 16, (sizeof(Ta) == 1) ? 64 : 32}};
  static constexpr int num_combinations =
      sizeof(combinations) / sizeof(combination);
};

// General query:
// types are not given, no default sizes and no implicit matrix construction
template <int M, int N, int K>
struct tpu_params<tpu::amx, void, void, void, M, N, K> {
  static constexpr std::size_t defaultM = -1; // depends on the type
  static constexpr std::size_t defaultN = -1;
  static constexpr std::size_t defaultK = -1;

  bool dynamic_p = false; // should be true in future implementations because
                          // AMX hardware supports dynamic sizes
  uint32_t numtiles = 8;
  scope_t scope = scope_t::sub_group;
  struct combination {
    uint32_t max_msize;
    uint32_t max_nsize;
    uint32_t max_ksize;
    matrix_type atype;
    matrix_type btype;
    matrix_type ctype;
    uint32_t msize;
    uint32_t nsize;
    uint32_t ksize;
  };
  using mt = matrix_type;
  static constexpr combination combinations[] = {
      {16, 16, 64, mt::sint8, mt::sint8, mt::sint32},
      {16, 16, 64, mt::sint8, mt::uint8, mt::sint32},
      {16, 16, 64, mt::uint8, mt::sint8, mt::sint32},
      {16, 16, 64, mt::uint8, mt::uint8, mt::sint32},
      {16, 16, 32, mt::bf16, mt::bf16, mt::fp32}};
  static constexpr int num_combinations =
      sizeof(combinations) / sizeof(combination);
};


enum class tpu {
  dpas,
  amx
};

enum class matrix_type {
bf16,
fp16,
fp19,  // tfloat32
fp32,
fp64,
sint2,
sint4,
sint8,
sint16,
sint32,
sint64,
uint2,
uint4,
uint8,
uint16,
uint32,
uint64
};

enum class scope_t {
sub_group,
work_group
};
}
```


=== Valid or not Example:
```c++
// User can provide sizes and tpu_params can assert if they are supported or not
using myparams = tpu_params<tpu::dpas, int8_t, int8_t, int, 8, 8, 32>;  
// use this to construct the ranges on the host side  
size_t NDRangeM = M / myparams::M;  
size_t NDRangeN = N / myparams::N;
// if M,N,K do not multiply the default sizes, padding has to be inserted 
// device code: the matrices are constructed using the default dimensions  
myparams::joint_matrix_a<sub_group> sub_a(sg);  
myparams::joint_matrix_b<sub_group> sub_b(sg);  
myparams::joint_matrix_c<sub_group> sub_c(sg);
```

=== Default Values Example:
```c++
using myparams = tpu_params_both<tpu::dpas, int8_t, int8_t, int>;  
// use this to construct the ranges on the host side  
size_t NDRangeM = M / myparams::defaultM;  
size_t NDRangeN = N / myparams::defaultN;
//if M,N,K do not multiply the default sizes, padding has to be done 
// device code: the matrices are constructed using the default dimensions  
myparams::joint_matrix_a<sub_group> sub_a(sg);  
myparams::joint_matrix_b<sub_group> sub_b(sg);  
myparams::joint_matrix_c<sub_group> sub_c(sg);

```

=== General Query Example:
```c++
M = 1500; // with msize = 8 and msize = 4, 
          // M can be broken up to 125 sequence of 8-sized ops and remaining 500 using 125 sequence of 4-sized ops
constexpr myparams params = myparams();
constexpr int msize = break_dimension(params.combinations, M);
constexpr int msize_remainder = break_dimension_remainder(params.combinations, M);
constexpr int nsize = params.combinations[0].nsize;
constexpr int ksize = params.combinations[0].ksize;
// device code:
joint_matrix<int8_t, msize, ksize> sub_a(sg);
joint_matrix<int8_t, ksize, nsize, matrix_layout::packed_b> sub_b(sg);
joint_matrix<int, msize, nsize> sub_c(sg);
//Remainder handling
```

//No need to provide more details in this section because the query interface can serve this. 

//## Implementation Status

//### oneAPI 2022.0 release
//For oneAPI 2022.0 release, a JIT implementation has been made available on both AMX and DPAS hardware of the specific features discussed above. In this case, there is no need to specify any architectural options to the command line. The static query interface can be used to guide the usage of this API. 
// The DPAS and AMX implementations support the logical capability support of the HW




## Future-looking API

### Matrix Initialization: `joint_matrix_fill`
The current interface presented above assumes that all the matrices are directly loaded from memory. This new function called `joint_matrix_fill`  makes it possible to multiply a matrix which is not directly loaded from memory but rather initialized directly in the register. On AMX, if the initialization constant is zero, this would map to `_tile_zero` intrinsic: 

```c++
namespace sycl::ext::oneapi::experimental::matrix {
  template <typename Group, typename T, size_t NumRows, size_t NumCols,
          matrix_layout L>
  void joint_matrix_fill(Group sg, joint_matrix<T, NumRows, NumCols, L, Group> &m, const T& v);
}
```

### Element Indexing and Element-Wise Operations 
There were multiple options on how to enable this feature.

#### Option 1: Non-restrictive element indexing
Allowing non-restrictive element indexing on the matrix element as shown below would result into slow indexing on the GPU.
 Besides, it will rely heavily on spirv and compiler vectorization:

```c++
matrix<int, 8, 8> C;
for (int i = 0; i < 8; i++) 
 for (int j = 0; j < 8; j++)
   C(i,j) *= alpha; //Align with mdspan
```
#### Option2: Restrictive fast element indexing 
In the DPC++ context, the expectation is that all element-wise operations will happen in a converged control path by all work items in the group.
Option 2 proposes a new set of element-wise operations by overloading existing operations to work on `matrix` object. An example is shown below:
```c++
joint_matrix<ONEAPI::sub_group, int, 8, 8> C(sg);
  C *= alpha; 
```
The problem with this option is that it is restrictive to a very limited set of operations. 

#### Option3: Restrictive conversion in the interface from SIMD to SIMT
Nvidia wmma interface added a new member to `fragment` class to designate the WI owned part of the matrix. 
While this provides fast element indexing on the GPU compared to the non-restrictive option, the user does not know the mapping of the owned data to the original matrix. This puts restriction on the user to implement new operations like sum of rows of a matrix for quantized algorithms.

#### proposal: Explicit conversion in the interface from SIMD to SIMT
We introduce a new function `get_wi_slice` that provides any portion of the matrix that the user wants but in a SIMT array object:.

```c++
namespace sycl::ext::oneapi::experimental::matrix {
template <typename Group, typename T, size_t NumRows, size_t NumCols, matrix_layout L>
  marray<T, n_rows * n_cols> get_wi_slice(joint_matrix<T, NumRows, NumCols, L, Group> &m, size_t row_index,  
                                          size_t col_index, size_t n_rows, size_t n_cols);
}
```

Example where each WI gets 1 column:  
```c++
marray<T,msize> wi_C = get_wi_slice(C, 0, wi_idx, msize, 1, matrix_layout::row_major);
for (int i = 0; i < msize; i++)        
   row_sum += wi_C[i];
```


### Memory scope
The current experimental API uses `joint_` semantics to define the memory scope of the matrix. The long term solution is to use the proposed https://github.com/intel/llvm/blob/sycl/sycl/doc/extensions/LocalMemory/SYCL_INTEL_local_memory.asciidoc[`group_local_memory` extension] to allocate the matrix in local memory associated with a SYCL group as shown in the example below.


```c++
multi_ptr<matrix<T>, address_space::local_space> tA_ptr = group_local_memory<matrix<sub_group, int8_t, tM, tN>>(sg);
```
We did not utilize this extension for this matrix API version because sub-group local memory is not yet well defined in {dpcpp}. Moreover, the representation of this notion in LLVM IR and SPIR-V is not clear yet. 


## Open Questions
- Besides row, col major and packed (VNNI) layout, what are the additional layouts that should absolutely be added?
- Are there alternative names for the `packed_a` and `packed_b` layouts that would be clearer to distinguish between the VNNI Layout in matrix A and VNNI layout in matrix B of a matrix multiply and add operation on AMX?
- Ronan Keryell: "It would be interesting to investigate whether providing also member functions would simplify the API. Provide both so it is possible to use the best one for each use case, while waiting for https://en.wikipedia.org/wiki/Uniform_Function_Call_Syntax to land into C++?"
- What should the API description include: (1) only features that are implemented, (2) features that are actually part of the API: currently implemented and the ones that we expect implementing them in the future. Specifically, should the document include things like dynamic_ extent and Group? These are part of the API but are not currently implemented.

## TODO List
- Add support for fill matrix and element-wise operations features
- Add 'matrix_use' parameter to the matrix to distinguish between matrix A, B, and matrix accumulator. This is necessary for supporting VNNI and transpose transform 

## Revision History

[frame="none",options="header"]
|======================
|Rev |Date       |Author     |Changes
|1   |2021-04-13 |Dounia Khaldi |Initial public working draft.
|2   |2021-10-05 |Dounia Khaldi |JIT implementation on both AMX and DPAS
|======================
